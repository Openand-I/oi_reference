# oi_reference

**oi - Open Image format proposal**

Uncompressed bit map format for adaptive palette quantised images - often resulting in image/frame sizes 30-40% smaller than PNG and comparable to crushed adaptive palette PNG, close to the theoretical minimum file size for (slightly lossy) lossless images and very high fps ( due to it being an uncompressed bitmap that one can simply display directly to screen, better than PNG, MPEG etc )

No-library display ( what other codecs call decoding/decompression ) ie simply display the frame/web canvas element/HTML image with few lines of in-page code, be it C, or Javascript or Webassembly by just writing the bits as presented by the .bitmap file ( with color lookup involved from an array of 1024 colors ) pixel by pixel to the canvas/HTML image. Further performance gains by doing this operation in blocks of one row at a time or similar, but initially, just writing the bit map on page pixel by pixel can be achieved in a few lines of in-page code.

Compression possible gives further ~10-15% reduction in file size ( may result in a decompression overhead and lower fps ). If there are loads of repeating pixel colors or transparent pixels, compression will help more, one can even try to allot color 00000000000 to the most common pixel color. It is recommended not to use compression unless doing client end upscaling so processing can be a single process and not to lose the high fps ( and no-library display ) to only 10-15% compression.

**Dual license** - GPLv2 ( can be used unencumbered in open source software, please have some sort of reference to this, it should not be completely hidden use with no references to the GUIDs or these format styles, "oi", ".bitmap" or this or similar pages ) and commercial ( please contact ).

There are at least a hundred useful variations of this bit map store style and one shall try to start all .bitmap files with two 64-bit GUIDs ( all sharing the first GUID ) stating that format is specific to the second level GUID. This format is roughly designed (attempted ) to directly load from file as almost a C data struct and ideally not even a loop should be required to populate the basic image data. Then one can avail of techniques similar to certain OS adaptive palette image display techniques, including fast access to memory locations and direct display to frame buffer. eg - a different GUID format may use little endian bitmaps so that the file loads to memory exactly as C would read it and even further loads to assembly language processing of even the simplest of operations like bitstring to int. Thus allowing for very high frame rates.

First line of this format is a GUID ( no magic numbers, it is not sustainable in the long run ). One can easily allow browsers to detect this first GUID and download the "weights" or "small code" to display the image. It is thereby requested to use different second level GUIDs  for one's own uses if using a different variation of this .bitmap format. The first level GUID can remain the same for browsers to detect the family of "display code" and the second level GUID to specify the different implementation with different weights and bit sizes etc. The first level GUID will only be decided upon once browsers and office suites take up the idea ( for embedding into documents etc ). So even that ( first GUID ) is currently considered an alpha version GUID. To repeat, keep the second GUID same if the format doesn't change ( even if your implementation is completely different ), and change the second GUID if your implementation even changes the file format slightly to suit your purposes. Variables like image size and bit depth can change without it being considered a file format change. Since the first level GUID is the same across all families, different browsers and office suites know which family of codes to load/download on the fly. One can then think of signed codec competitions that rely on healthy one-upmanship to display the second level GUID based files. Browser will probably store it's reference implementation or download the best signed implementation updates from a remote location. This allows for better codec development and optimisation styles and chaining so that the file is not copied once for every code/codec that wants to manipulate it in some way. Hopefully even pass void * streams between Javascript and Webassembly one day. 

Do note that bitwise data structure manipulations, even in-place modifications (eg for the lookup and replace of actual colors ) are very efficient often achieving nanosecond level latency. One can even try and design a negative index bitwise data type so that replaces happen to the left going into negative index territory and the web download stream thread is writing to the right ( with no change in it's needle location ) and even a display thread followed by a dynamic array reduce thread after having displayed the frame. This can allow for near zero memory required to display a streaming movie ( say not more than two rows or two frames of data, more like 2 seconds or 2 minutes ). Again it is not sustainable to have single monolithic servers in the long run and multiple peers will serve the data, so a threaded and aware receiving end with low memory requirements is key to content sustainability. This issue is further visible in what can only be termed as "malloc" paranoia by all websites, choosing to allocate as much memory as possible lest they do not get that during operation. Clearly RAM requirements of 2-3GB just to start the browser and load a few websites is too much. And one can even have dynamic DLL technology to be able to support thousands of codecs with zero runtime memory requirements. Or a Red Black Tree of codecs loaded from memory/disk so that common codecs remain high up the red black tree. XML gave way to JSON because of memory requirements, it need not be the same for everything, even HTML pre processing to pre-lookup all the site's DNS entries ( again can be a commented table ). One can even imagine serving quantised images ( at 35-50% smaller sizes ) and then refreshing the images in place with full fidelity images. 

Stepping back, the basic logic here is that one loads an "adaptive palette" image where the pixels contain not the exact color information but a lookup index in a palette file ( or header ), and then simply looks up the color and displays it. The advantage being that 10-bits per pixel required to store is much lesser than 24-bits per pixel in store. The lookup table can store the 24-bit color and even upgrades to 30-bit color do not result in a higher file size ( around 0.5K to 1K ) increase in the palette size, which is itself around 3K per file. Even changes to CMYK ( 4-dimensions ) or 30-bit = 10-bit decimal numbers ( 0 - 1 ) for RGB levels only increase the header from 3K to around 4K, but **show no change in the file size**.

Current reference implementation has chosen a quantisation level of 1024 unique colors to be able to have not more than a very slight perceptible difference in image quality using some very simple quantisation techniques like gamma of 1.01+Quantisation to 1024 colors + Floyd Steinberg Dithering. Again this is highly dependant on the quality of the quantisation technique and in an ideal world quantisation would be an image editing variable before one even saves the image, ie the image tool would dynamically present only the top 1024 colors for the image and every edit would cause it to recalculate all 1024 colors and repaint the image for the editor to see before saving. The quality standard applied here is the poor man's display standard, similar to streaming to developing markets with low bandwidth. The reputation for the technique of quantisation only suffered by the 8-bit 256 color palette due to the MS Windows limitations for Adaptive Palette animations, and does not suffer from the same with 1024 colour ( 10-bit ) palettes with a good quantisation algorithm, reaching very close to human imperceptibility ( around a 1% degradation visible in 2x zoom ). Furthermore all tools did not support the 10-bit palette and one has to manually use tools like pngcrush (after quantisation) to achieve similar savings in file sizes. 1024 colors is slightly too low for even the poor man and 4096 results in a very big header per file and even file data size. This also allows for "naked frame" ( no color palette per frame just a body ) movies/animations to use one palette file ( 3Kb ) to render an entire scene ( assuming one scene or shot sequence has the same color palette ). After quantisation losses, it can be said that the storage is a lossless format.

It is parallel and seek friendly as the exact pixel can be located in memory just by multiplying by 10. Basically it's just 1024x24 bits with the 1024 color palette followed by image width x image height x 10 bits ( starting from the top left ) with the image data each 10-bit string just being a lookup of the color palette, no padding. Or a bit map. 

Do refer to the NetPBM style of format definition with a text header and binary data ( P6 ) for similarities in thinking, the only difference is that the binary data has to obtain the color lookup information from somewhere. And each subsequent bit chunk of 10-bits is the entire information for our proposed pixel not 8x3 bits.

The ICC color profile is a 64K header ( compared to 3K for our .bitmap ) and not even a color palette from the looks of it and one can even imagine using a different color palette during night and day and ambient light sensor environments. One can see deep inside the PNG library to see their use of the palette with quantised images, this functionality ( the file size improvement resultant from using a color palette ) is not enabled by default and is done by external tools like pngcrush. Again an option for the image editor software to have a "quantise-first" thinking or working with the 16M colors just using not more than 1024 and recalculating the entire image color set on every edit ( certainly not forcing the user to use not more than 1024 colours ). 
